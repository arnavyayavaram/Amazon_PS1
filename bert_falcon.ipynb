{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open('title_refrigerator3.csv', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('labelled_jumbled2.csv', 'w', newline = '')\n",
    "csvreader = csv.reader(f)\n",
    "csvwriter = csv.writer(f2)\n",
    "lines = list(csvreader)[1:]\n",
    "# example = lines[0][1]\n",
    "csvwriter.writerow(['text', 'labels'])\n",
    "for line in lines:\n",
    "    labels = []\n",
    "    text = line[1]\n",
    "\n",
    "    char_replace = ['(', ')', '|', '<', '>', '\\'', '\"', '*', '\\n']\n",
    "    for char in char_replace:\n",
    "        text=text.replace(char, ' ')\n",
    "    # text = text.replace('(','')\n",
    "    # text = text.replace(')','')\n",
    "    text = text.strip(\",.- \"\"\")\n",
    "    if(text == \"\"):\n",
    "        continue\n",
    "    brand = line[2]\n",
    "    model = line[3]\n",
    "    num_brand_words = len(brand.split())\n",
    "    start_index_brand = text.find(brand)\n",
    "    num_model_words = len(model.split())\n",
    "    start_index_model = text.find(model)\n",
    "    brand_start = 0\n",
    "    model_start = 0\n",
    "    for word in text.split():\n",
    "        if(word == brand.split()[0]):\n",
    "            labels.append('B-br')\n",
    "            brand_start = 1\n",
    "        elif(word == model.split()[0]):\n",
    "            labels.append('B-mo')\n",
    "            model_start = 1\n",
    "        elif (num_brand_words > 1 and brand_start == 1):\n",
    "            labels.append('I-br')\n",
    "            num_brand_words -= 1\n",
    "        elif (num_model_words > 1 and model_start == 1):\n",
    "            labels.append('I-mo')\n",
    "            num_model_words -= 1\n",
    "        else:\n",
    "            labels.append('O')\n",
    "    csvwriter.writerow([text, ' '.join(labels)])\n",
    "\n",
    "    temp = list(zip(text.split(), labels))\n",
    "    random.shuffle(temp)\n",
    "    res1, res2 = zip(*temp)\n",
    "    res1, res2 = list(res1), list(res2)\n",
    "    csvwriter.writerow([' '.join(res1), ' '.join(res2)])\n",
    "\n",
    "    temp = list(zip(text.split(), labels))\n",
    "    random.shuffle(temp)\n",
    "    res1, res2 = zip(*temp)\n",
    "    res1, res2 = list(res1), list(res2)\n",
    "    # label_str = ' '.join(res2)\n",
    "    csvwriter.writerow([' '.join(res1), ' '.join(res2)])\n",
    "\n",
    "    index = random.randint(0, len(labels)-1)\n",
    "    del res1[index]\n",
    "    del res2[index]\n",
    "    csvwriter.writerow([' '.join(res1), ' '.join(res2)])\n",
    "f2.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bosch 347 L Frost Free Double Door 3 Star Refr...</td>\n",
       "      <td>B-br O O O O O O O O O B-mo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Double Refrigerator 3 Bosch Frost L KDN43VL40I...</td>\n",
       "      <td>O O O B-br O O B-mo O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347 Frost 3 KDN43VL40I Bosch L Star Door Free ...</td>\n",
       "      <td>O O O B-mo B-br O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347 Frost 3 Bosch L Star Door Free Refrigerato...</td>\n",
       "      <td>O O O B-br O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bosch 415 L Frost Free Double Door 3 Star Refr...</td>\n",
       "      <td>B-br O O O O O O O O O B-mo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Bosch 347 L Frost Free Double Door 3 Star Refr...   \n",
       "1  Double Refrigerator 3 Bosch Frost L KDN43VL40I...   \n",
       "2  347 Frost 3 KDN43VL40I Bosch L Star Door Free ...   \n",
       "3  347 Frost 3 Bosch L Star Door Free Refrigerato...   \n",
       "4  Bosch 415 L Frost Free Double Door 3 Star Refr...   \n",
       "\n",
       "                        labels  \n",
       "0  B-br O O O O O O O O O B-mo  \n",
       "1  O O O B-br O O B-mo O O O O  \n",
       "2  O O O B-mo B-br O O O O O O  \n",
       "3       O O O B-br O O O O O O  \n",
       "4  B-br O O O O O O O O O B-mo  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('labelled_jumbled2.csv')\n",
    "df = df[df['labels'].notnull()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-br', 'O', 'B-mo', 'I-mo', 'I-br'}\n"
     ]
    }
   ],
   "source": [
    "# Split labels based on whitespace and turn them into a list\n",
    "\n",
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    " \n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-br', 1: 'B-mo', 2: 'I-br', 3: 'I-mo', 4: 'O'}\n",
      "{'B-br': 0, 'B-mo': 1, 'I-br': 2, 'I-mo': 3, 'O': 4}\n"
     ]
    }
   ],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(ids_to_labels)\n",
    "print(labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush Bhauwala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "# text_tokenized = tokenizer(example, padding = 'max_length', max_length = 100, return_tensors = 'pt')\n",
    "# print(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def align_label(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=100, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        txt = df['text'].values.tolist()\n",
    "        self.texts = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = 100, truncation=True, return_tensors=\"pt\") for i in txt]\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1335/1335 [16:55<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Loss:  0.199 | Accuracy:  0.948 | Val_Loss:  0.080 | Accuracy:  0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1335/1335 [16:46<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Loss:  0.098 | Accuracy:  0.976 | Val_Loss:  0.058 | Accuracy:  0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1335/1335 [16:43<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Loss:  0.073 | Accuracy:  0.981 | Val_Loss:  0.047 | Accuracy:  0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1335/1335 [16:39<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Loss:  0.056 | Accuracy:  0.984 | Val_Loss:  0.042 | Accuracy:  0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1335/1335 [16:37<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Loss:  0.063 | Accuracy:  0.981 | Val_Loss:  0.081 | Accuracy:  0.969\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "def train_loop(model, df_train, df_val):\n",
    "\n",
    "    train_dataset = DataSequence(df_train)\n",
    "    val_dataset = DataSequence(df_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    # print(1)\n",
    "    # a, b = next(iter(train_dataloader))\n",
    "    # print(a)\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        # print('test')\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "        # print('test4')\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "            # print('test2')\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "              # print('test3')                \n",
    "              logits_clean = logits[i][train_label[i] != -100]\n",
    "              label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_train += acc\n",
    "              total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_data, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][val_label[i] != -100]\n",
    "              label_clean = val_label[i][val_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_val += acc\n",
    "              total_loss_val += loss.item()\n",
    "              # print('test1')\n",
    "\n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "model = BertModel()\n",
    "train_loop(model, df_train, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.968\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, df_test):\n",
    "\n",
    "    test_dataset = DataSequence(df_test)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0.0\n",
    "\n",
    "    for test_data, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "            input_id = test_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            loss, logits = model(input_id, mask, test_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][test_label[i] != -100]\n",
    "              label_clean = test_label[i][test_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_test += acc\n",
    "\n",
    "    val_accuracy = total_acc_test / len(df_test)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "\n",
    "\n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 4, 4, 2, 4, 1, 2, 4, 0, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 1, 4], [4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 0, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [1, 3, 3, 0, 4, 3, 3], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3, 3], [4, 0, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4], [1, 4, 4, 4, 4, 4, 0, 4, 4], [0, 4, 4, 4, 4, 4, 4, 3, 4, 1, 4, 4], [4, 4, 0, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 1, 4, 4, 0, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 1, 0, 4, 4], [4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 0, 1], [4, 4, 4, 1, 4, 4, 0, 4, 4, 3, 4, 4], [4, 4, 4, 4, 4, 4, 4, 1, 4, 0], [4, 4, 1, 4, 3, 0, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 0, 4, 4, 4, 3, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 0, 1, 4, 4, 4, 4], [4, 4, 1, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 1, 4, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 0, 3], [0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 1, 4, 4, 0, 3, 3, 3, 4, 4, 4, 4, 4, 4], [4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 0, 1], [4], [4, 4], [4, 4, 0, 4, 4, 4, 4, 4, 1, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4, 1, 0, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4], [4, 1, 4, 4, 0, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 0, 4, 4], [4, 3, 4, 4, 4, 0, 3, 1, 4, 3, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3], [4, 4, 4, 0, 4], [4, 4, 4, 1, 4, 4, 4, 4, 4, 0], [4, 4, 0, 4, 4, 4, 4, 4, 4], [1, 0, 2, 2, 4, 4, 4, 4, 4], [4, 0, 4, 4, 1, 4, 4, 4, 4, 4], [4, 3, 4, 1, 4, 0, 4, 4, 4, 4, 3, 4, 4], [4, 4, 0, 4, 4, 4, 4, 4, 4, 1, 4], [4, 4, 3, 4, 1, 4, 4, 4, 4, 4, 0, 4], [4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4], [4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 1, 4], [1, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 3, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 3, 4, 0, 4, 4, 4, 3], [4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4], [0, 1, 4, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4], [2, 4, 4, 1, 4, 0, 4, 4, 4, 4], [4, 4, 4, 4, 0, 1, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3], [0, 1, 4, 4, 4, 4, 4, 4, 4, 4], [4, 1, 0, 4, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 1, 4, 4, 0, 4, 4, 4], [4, 1, 4, 4, 4, 4, 4, 0, 4, 4, 4], [4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 0, 4, 4, 4, 3, 4, 1], [0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4], [4, 4, 4, 0, 4, 4, 4], [4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 0, 1, 4, 4, 4, 4], [0, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 1], [4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 1, 4, 4, 4, 0, 4, 4], [4, 2, 4, 4, 2, 4, 1, 4, 4, 4, 0, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4], [4, 4, 0, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 3, 4, 4, 4, 4, 1, 4], [0, 4, 1, 4, 4, 4, 3, 4, 3, 4, 3, 4, 3, 4, 4], [4, 4, 4, 4, 1, 0, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 1], [4, 4, 3, 0, 4, 1, 4, 4, 4, 4, 4, 4], [1, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3, 3], [4, 4, 4, 4, 4, 3, 4, 1, 4, 4, 0, 4], [4, 1, 4, 4, 4, 4, 4, 0, 4, 4, 4], [1, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 3, 4, 1, 4, 4, 4, 4, 4, 0, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 1, 4, 4, 4, 4, 4, 0, 4], [3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 4, 3], [1, 0, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 1, 4, 4, 4, 4, 4], [0, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 1], [1, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4], [1, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 3, 4], [4, 1, 4, 4, 0, 4, 4, 4, 4, 4], [4, 4, 4, 0, 4, 4, 3, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 0, 4, 4, 1, 4, 4, 4, 4, 4, 4], [4, 4, 4, 0, 4, 4, 1, 4, 4, 4, 4], [4, 4, 4, 4, 0, 1, 3, 3], [4, 4, 4, 4, 0, 4, 1, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 0, 4, 4, 1], [4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 1, 0, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 1, 4, 0, 4, 4, 4, 4, 4], [0, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 1, 4, 4, 4, 4, 0, 4], [4, 4, 4, 4, 4, 1, 0, 4, 4, 4], [4, 0, 4, 4, 4, 2, 4, 1, 4, 4, 4, 4], [4, 4, 4, 4, 1, 4, 0, 4, 4, 4], [4, 4, 4, 4, 4, 4, 0, 4, 1], [4, 4, 4, 4, 0, 4, 4, 1, 4, 4, 4], [3, 1, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 0, 4, 4, 3, 3], [0, 4, 4, 4, 4, 4, 4, 4, 4, 1], [1, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3], [0, 4, 2, 4, 4, 4, 4, 1, 4, 4, 4], [4, 4, 4, 4, 4, 4, 1, 4, 4, 0, 4, 3], [1, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 1], [1], [4, 4, 4, 4, 4, 0, 4, 4, 2, 4], [4, 4, 4, 0, 4, 4, 4, 4, 1, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 3, 0, 4, 3, 4, 1, 3], [0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1], [3, 4, 3, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 3], [4, 4, 4, 4, 4, 4, 4, 1, 0, 4, 4], [4, 4, 1, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 1, 4], [4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 0], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4], [4, 4, 1, 4, 4, 4, 4, 0, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 0], [4, 4, 4, 4, 4, 4, 4, 4, 1, 4], [0, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 1, 4, 4, 4, 4, 4, 4, 4, 0, 4], [4, 0, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 0, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 1], [4, 0, 4, 1, 2, 4, 4, 4, 4, 4, 4, 2], [4, 4, 0, 4, 4, 1, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 3, 3, 4], [4, 4, 4, 4, 4, 0, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 0, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4], [1, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4], [4, 1, 4, 4, 4, 4, 4, 0, 4, 4], [4, 4, 4, 4, 4, 4, 4, 0, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 0, 4, 4, 1, 4], [4, 4, 4, 4, 0, 4, 4, 4, 1, 4, 4], [4, 4, 4, 4, 3, 4, 4, 4, 4, 0, 4], [0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 1], [4, 1, 4, 4, 4, 4, 4, 4, 4, 0, 4], [4, 4, 4, 0, 4, 4, 4, 4, 1, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 0, 4, 4, 1, 4, 4, 4, 3], [3, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 3, 1, 4, 0], [4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4], [3, 4, 4, 1, 4, 4, 3, 4, 4, 4, 3, 4, 3, 4, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 0, 4, 4, 1, 4, 4], [4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 1, 2], [4, 4, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 1], [0, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 0], [4, 4, 4, 4, 4, 0, 4, 4, 4, 4], [4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 0, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 0, 4, 4, 4, 1, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 1, 4, 4, 4, 4, 4, 0, 4, 4], [0, 1, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [3, 4, 4, 4, 4, 3, 4, 4, 4, 1, 0, 4, 4], [4, 4, 1, 4, 0, 4, 4, 4, 4], [4, 4, 4, 4, 0, 4, 4, 4, 1, 4], [1, 4, 4, 0, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3], [4, 4, 4, 4, 4, 4, 4, 1, 3, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 0, 4, 4, 4, 4, 1, 4, 3, 4, 4, 3, 4, 3, 3], [0, 4, 4, 4, 4, 4, 0, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4], [3, 3, 4, 4, 0, 4, 4, 1, 3, 4, 4, 3, 4, 4, 4], [4, 4, 4, 4, 4, 0, 1, 4, 4, 4, 4, 4], [4, 4, 4, 4, 3, 4, 3, 1, 0, 3, 4, 4, 4], [4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 2, 4, 4, 4, 4, 4, 4, 1, 2, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3, 4, 4], [4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4], [4, 4, 4, 4, 0, 4, 4, 4, 1], [4, 3, 1, 4, 4, 3, 4, 4, 4, 4, 0, 3, 4], [0, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 0, 4, 4, 4, 1, 4, 4, 4], [4, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [0, 3, 4, 4, 3, 1, 4, 4, 4, 4, 3, 3, 4, 4, 4], [0, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 1, 4, 4, 4, 4, 4, 0, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 0, 4, 4, 4, 1, 4, 3, 4, 4, 3, 4, 3, 3], [0, 1, 3, 3, 3, 4, 4, 4], [4, 4, 4, 0, 4, 1, 4, 4, 4, 4], [4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0], [4, 4, 0, 4, 4, 4, 1, 4, 4, 4], [0, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 1, 4, 4, 4, 4, 4, 4, 0, 4, 4, 3], [1, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4, 0, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4], [3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 4, 0, 1, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 4, 0, 1, 4], [4, 0, 4, 4, 1, 4, 4, 4, 4, 4], [4, 4, 4, 1, 4, 4, 4, 4, 0, 4], [4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 1, 2, 4, 4, 4, 4, 4, 0], [4, 1, 4, 4, 4, 4, 0, 4, 4, 4, 4], [4, 0, 4, 4, 1, 4, 4, 4, 4, 4, 4], [0, 4, 4, 1, 4, 4, 4, 4, 4, 4], [0, 1], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 3, 4, 0, 4, 4, 4, 4, 3, 1, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 2, 4, 0, 2, 4, 4, 4, 4, 4, 4, 1, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 0, 4, 4, 4, 4, 4, 4, 1, 4, 4], [4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4], [4, 4, 4, 1, 4, 0, 4, 4, 4, 2, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4], [4, 4, 4, 4, 0, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 0, 1, 4, 4, 4, 4, 4, 4], [1, 4, 4, 0, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 0, 1], [0, 4, 4, 2, 4, 4, 4, 4, 1, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4], [4, 1, 4, 0, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 3, 4, 4, 1, 4, 4, 0, 4, 4], [4, 4, 4, 1, 4, 0, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4], [1, 4, 4, 4, 0, 4, 4, 4, 3, 3, 4, 3], [0, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4], [4, 3, 3, 3, 0, 4, 4, 4, 1, 4, 4, 4, 3, 4, 4], [1, 0, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 2, 1, 4, 0, 4, 4], [4, 4, 4, 4, 4, 1, 0, 4], [4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3, 3], [4, 0, 4, 4, 4, 4, 4, 1, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 3, 3], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4], [1, 4, 4, 4, 4, 4, 4, 0, 4, 4], [4, 4, 4, 0, 4, 4, 1, 4, 4], [4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4], [4], [4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 0, 4, 4, 4, 4, 4, 4, 1, 4], [4, 4, 4, 0, 4, 4, 4, 4, 4, 1, 4], [4, 1, 4, 4, 4, 4, 0, 4, 4], [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4], [3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 1], [0, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 1, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4], [4, 4, 0, 4, 4, 4, 1, 4, 4, 4, 4, 3], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 0, 4, 4, 4], [1, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 4], [4, 4, 4, 1, 4, 0, 4, 4, 4, 4], [0, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1], [4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 1, 4, 4, 4, 4, 0, 4], [2, 4, 4, 4, 4, 4, 4, 1, 0, 4, 2, 4], [1, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 3, 4], [4, 4, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 1, 4, 4], [4, 4, 4, 4, 4, 4, 0, 4, 1, 4], [4, 4, 4, 4, 4, 4, 0, 4, 4, 4], [4, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4], [4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 4], [4, 4, 0, 1, 4, 4, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 1, 3], [0, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [3, 3, 1, 4, 0, 4, 4, 4, 4, 4, 4, 3, 4, 4], [4, 4, 4, 4, 4, 2, 4, 1, 4, 4, 4, 0], [1, 3, 4, 3, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4], [4, 4, 0, 1, 4, 4, 4, 4, 4, 4], [4, 4, 0, 4, 4, 4, 1, 4, 4, 4, 4], [4, 4, 1, 4, 4, 4, 4, 4, 4, 0], [4, 4, 4, 4, 4, 4, 1, 4, 4, 0, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 0, 4, 2, 4, 4, 4, 2, 4, 4, 4, 1], [4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 1, 0, 4], [4, 1, 4, 4, 0, 4, 4, 4, 4, 4], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3], [4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]\n",
      "[['O', 'O', 'O', 'I-br', 'O', 'B-mo', 'I-br', 'O', 'B-br', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-br', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-mo', 'I-mo', 'I-mo', 'B-br', 'O', 'I-mo', 'I-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo'], ['O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'B-mo', 'O', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O', 'I-mo', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'B-br'], ['O', 'O', 'B-mo', 'O', 'I-mo', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'I-mo', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-mo', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'I-mo', 'I-mo', 'O', 'B-br', 'I-mo'], ['B-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-mo', 'O', 'O', 'B-br', 'I-mo', 'I-mo', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'I-br', 'O', 'O', 'I-br', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo'], ['O'], ['O', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O'], ['O', 'I-mo', 'O', 'O', 'O', 'B-br', 'I-mo', 'B-mo', 'O', 'I-mo', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo'], ['O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-mo', 'B-br', 'I-br', 'I-br', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O'], ['O', 'I-mo', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['O', 'O', 'I-mo', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'I-br', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'I-mo', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'B-br', 'O', 'O', 'O', 'I-mo'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'B-mo', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'I-mo', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O'], ['I-br', 'O', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo'], ['B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-mo', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'I-br', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'I-mo', 'O', 'B-mo'], ['B-br', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['O', 'O', 'O', 'I-mo', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['O', 'I-br', 'O', 'O', 'I-br', 'O', 'B-mo', 'O', 'O', 'O', 'B-br', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['B-br', 'O', 'B-mo', 'O', 'O', 'O', 'I-mo', 'O', 'I-mo', 'O', 'I-mo', 'O', 'I-mo', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo'], ['O', 'O', 'I-mo', 'B-br', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo'], ['O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'B-mo', 'O', 'O', 'B-br', 'O'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'I-mo', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['I-mo', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'I-mo'], ['B-mo', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'I-br', 'O', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O'], ['O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'B-mo', 'I-mo', 'I-mo'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'B-mo', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'I-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'O', 'O'], ['O', 'B-br', 'O', 'O', 'O', 'I-br', 'O', 'B-mo', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O'], ['I-mo', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'I-mo', 'I-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-mo', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo'], ['B-br', 'O', 'I-br', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'I-mo'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-mo'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'I-br', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'I-mo', 'B-br', 'O', 'I-mo', 'O', 'B-mo', 'I-mo'], ['B-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['I-mo', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'I-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'B-mo', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo'], ['O', 'B-br', 'O', 'B-mo', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'I-br'], ['O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'I-mo', 'I-mo', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-br', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['B-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'I-mo'], ['I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'I-mo', 'O', 'O', 'I-mo', 'B-mo', 'O', 'B-br'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-mo', 'O', 'O', 'B-mo', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'I-mo', 'O', 'I-mo', 'O', 'B-br'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-br'], ['O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['B-br', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-mo', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O'], ['B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-br', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'I-mo', 'O', 'O', 'I-mo', 'O', 'I-mo', 'I-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-mo', 'I-mo', 'O', 'O', 'B-br', 'O', 'O', 'B-mo', 'I-mo', 'O', 'O', 'I-mo', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'I-mo', 'O', 'I-mo', 'B-mo', 'B-br', 'I-mo', 'O', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-br', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'I-br', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo'], ['O', 'I-mo', 'B-mo', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'B-br', 'I-mo', 'O'], ['B-br', 'I-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['B-br', 'I-mo', 'O', 'O', 'I-mo', 'B-mo', 'O', 'O', 'O', 'O', 'I-mo', 'I-mo', 'O', 'O', 'O'], ['B-br', 'I-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'I-mo', 'O', 'O', 'I-mo', 'O', 'I-mo', 'I-mo'], ['B-br', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'B-mo', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O'], ['B-br', 'I-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'I-mo'], ['B-mo', 'O', 'I-mo', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-mo', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'I-mo', 'I-mo', 'O', 'O', 'B-br', 'B-mo', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O'], ['O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'B-mo', 'I-br', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'I-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'I-mo', 'B-mo', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'I-br', 'O', 'B-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'I-br', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O'], ['O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo'], ['B-br', 'O', 'O', 'I-br', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'I-mo', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O'], ['O', 'O', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'I-mo', 'I-mo', 'O', 'I-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['O', 'I-mo', 'I-mo', 'I-mo', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'I-mo', 'O', 'O'], ['B-mo', 'B-br', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'I-br', 'B-mo', 'O', 'B-br', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O'], ['O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo'], ['O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo', 'I-mo', 'I-mo', 'I-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'B-mo', 'O', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O', 'B-mo'], ['B-br', 'I-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-br', 'I-br', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'I-mo'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O'], ['B-br', 'I-br', 'I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['I-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'B-br', 'O', 'I-br', 'O'], ['B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'I-mo', 'O'], ['O', 'O', 'O', 'I-br', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'I-br', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'B-mo', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'B-br', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O'], ['O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-mo', 'I-mo', 'B-mo', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'I-mo', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'I-br', 'O', 'B-mo', 'O', 'O', 'O', 'B-br'], ['B-mo', 'I-mo', 'O', 'I-mo', 'O', 'O', 'B-br', 'I-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-br', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-br', 'O', 'I-br', 'O', 'O', 'O', 'I-br', 'O', 'O', 'O', 'B-mo'], ['O', 'O', 'O', 'I-br', 'O', 'O', 'I-br', 'O', 'O', 'O', 'B-mo', 'B-br', 'O'], ['O', 'B-mo', 'O', 'O', 'B-br', 'O', 'O', 'O', 'O', 'O'], ['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'I-mo'], ['O', 'B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "F1 score:  0.9075409836065573\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          br       0.81      0.93      0.86       348\n",
      "          mo       0.98      0.92      0.95       399\n",
      "\n",
      "   micro avg       0.89      0.93      0.91       747\n",
      "   macro avg       0.89      0.93      0.91       747\n",
      "weighted avg       0.90      0.93      0.91       747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to calculate precision and recall\n",
    "# {0: 'B-br', 1: 'B-mo', 2: 'I-br', 3: 'I-mo', 4: 'O'}\n",
    "# {'B-br': 0, 'B-mo': 1, 'I-br': 2, 'I-mo': 3, 'O': 4}\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def evaluate(model, df_test):\n",
    "    \n",
    "    test_dataset = DataSequence(df_test)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    for test_data, test_label in test_dataloader:\n",
    "\n",
    "        # print(test_data)\n",
    "        test_label = test_label.to(device)\n",
    "        mask = test_data['attention_mask'].squeeze(1).to(device)\n",
    "        input_id = test_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        loss, logits = model(input_id, mask, test_label)\n",
    "\n",
    "        for i in range(logits.shape[0]):\n",
    "            logits_clean = logits[i][test_label[i] != -100]\n",
    "            label_clean = test_label[i][test_label[i] != -100]\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "            true_labels.extend(label_clean.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "            # print(logits[i])\n",
    "            # print(label_clean.cpu().numpy())\n",
    "            y_true.append(label_clean.cpu().numpy().tolist())\n",
    "            y_pred.append(predictions.cpu().numpy().tolist())\n",
    "        # print()\n",
    "            # y_true.append(true_labels)\n",
    "            # y_pred.append(predicted_labels)\n",
    "    # print(len(y_true))\n",
    "    # print(y_true)\n",
    "    # for i in y_true:\n",
    "    #     print(i)\n",
    "\n",
    "    # precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    # recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    # f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    # print(f'Test Precision: {precision:.3f}')\n",
    "    # print(f'Test Recall: {recall:.3f}')\n",
    "    # print(f'Test F1 Score: {f1:.3f}')\n",
    "    # count = 0;\n",
    "    # for i in zip(predicted_labels, true_labels):\n",
    "    #     predicted_label, true_label = i\n",
    "    #     if(predicted_label == true_label):\n",
    "    #         count += 1;\n",
    "    # print(f'Test Accuracy: {val_accuracy:.3f}')\n",
    "    print(y_true)\n",
    "    for i in range(len(true_labels)):\n",
    "        true_labels[i] = ids_to_labels[true_labels[i]]\n",
    "\n",
    "    for i in range(len(predicted_labels)):\n",
    "        predicted_labels[i] = ids_to_labels[predicted_labels[i]]\n",
    "    \n",
    "    y_true_id=[]\n",
    "    y_pred_id=[]\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_id.append([])\n",
    "        for j in range(len(y_true[i])):\n",
    "            y_true_id[i].append(ids_to_labels[y_true[i][j]])\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred_id.append([])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            y_pred_id[i].append(ids_to_labels[y_pred[i][j]])\n",
    "    \n",
    "    print(y_true_id)\n",
    "        \n",
    "\n",
    "    \n",
    "    # cm = confusion_matrix(true_labels,predicted_labels, labels=['B-br', 'B-mo', 'I-br', 'I-mo', 'O'])\n",
    "    # # Plot the confusion matrix.\n",
    "    # sns.heatmap(cm,\n",
    "    #             annot=True,\n",
    "    #             fmt='g')\n",
    "    # plt.ylabel('Actual',fontsize=13)\n",
    "    # plt.xlabel('Prediction',fontsize=13)\n",
    "    # plt.title('Confusion Matrix',fontsize=17)\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    # # Finding precision and recall\n",
    "    # print('LABEL', 'PRECISION', 'RECALL')\n",
    "    # for i in range(cm.shape[0]):\n",
    "    #     pred_equal_true = cm[i][i]\n",
    "    #     predicted = 0\n",
    "    #     actual = 0\n",
    "    #     for j in range(cm.shape[0]):\n",
    "    #         predicted += cm[j][i]\n",
    "    #         actual += cm[i][j]\n",
    "    #     precision = round(pred_equal_true / predicted, 3)\n",
    "    #     recall = round(pred_equal_true / actual, 3)\n",
    "    #     print(ids_to_labels[i], precision, recall)\n",
    "    # accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    # print(\"Accuracy   :\", accuracy)\n",
    "    from seqeval.metrics import accuracy_score\n",
    "    from seqeval.metrics import classification_report\n",
    "    from seqeval.metrics import f1_score\n",
    "    print()\n",
    "    print(\"F1 score: \", f1_score(y_true_id, y_pred_id))\n",
    "    print()\n",
    "    print(classification_report(y_true_id, y_pred_id))\n",
    "\n",
    "\n",
    "evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bosch 347 L Frost Free Double Door 3 Star Refrigerator model KDN43VL40I offers efficient cooling and ample storage space for your food and beverages.\n",
      "['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    mask = text['attention_mask'].to(device)\n",
    "    input_id = text['input_ids'].to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "            \n",
    "evaluate_one_text(model, 'Bosch 347 L Frost Free Double Door 3 Star Refrigerator model KDN43VL40I offers efficient cooling and ample storage space for your food and beverages.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Midea MDRS619FGG28IND Frost-Free Side-by-Side Refrigerator, \"frost-free\" means the refrigerator does not have to be defrosted manually, as it has an auto-defrost feature.\n",
      "['O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "evaluate_one_text(model, 'In the Midea MDRS619FGG28IND Frost-Free Side-by-Side Refrigerator, \"frost-free\" means the refrigerator does not have to be defrosted manually, as it has an auto-defrost feature.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Midea MDRS619FGG28IND Frost Free Side by Side Refrigerator, the brand is listed as Midea and the model number is MDRS619FGG28IND.\n",
      "['O', 'O', 'B-br', 'B-mo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'O']\n"
     ]
    }
   ],
   "source": [
    "evaluate_one_text(model, 'In the Midea MDRS619FGG28IND Frost Free Side by Side Refrigerator, the brand is listed as Midea and the model number is MDRS619FGG28IND.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whirlpool 240 L Frost Free Multi-Door Refrigerator (FP 263D PROTTON ROY, German Steel)\n",
      "['B-br', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-mo', 'B-mo', 'O', 'O', 'O', 'B-br', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "evaluate_one_text(model, 'Whirlpool 240 L Frost Free Multi-Door Refrigerator (FP 263D PROTTON ROY, German Steel)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
