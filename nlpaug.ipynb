{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 82-83: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m augmented_text \u001b[39m=\u001b[39m aug\u001b[39m.\u001b[39maugment(text)\n\u001b[0;32m     15\u001b[0m \u001b[39m# print(\"Original:\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# print(text)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# print(\"Augmented Text:\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# print(augmented_text)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m csvwriter\u001b[39m.\u001b[39mwriterow([line[\u001b[39m0\u001b[39m], augmented_text[\u001b[39m0\u001b[39m], line[\u001b[39m2\u001b[39m], line[\u001b[39m3\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\Ayush Bhauwala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_encode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,encoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 82-83: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"MODEL_DIR\"] = '../model'\n",
    "import csv\n",
    "f1= open('title_refrigerator2.csv', 'r')\n",
    "f2= open('augmented_titles.csv', 'w', newline='')\n",
    "csvreader = csv.reader(f1)\n",
    "csvwriter = csv.writer(f2)\n",
    "lines = list(csvreader)[1:]\n",
    "csvwriter.writerow(['ID', 'Augmented_title', 'Brand', 'Model'])\n",
    "for line in lines:\n",
    "    text = line[1]\n",
    "    aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-cased', action=\"insert\")\n",
    "    augmented_text = aug.augment(text)\n",
    "    # print(\"Original:\")\n",
    "    # print(text)\n",
    "    # print(\"Augmented Text:\")\n",
    "    # print(augmented_text)\n",
    "    csvwriter.writerow([line[0], augmented_text[0], line[2], line[3]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Bosch 347 L Frost Free Double Door 3 Star Refrigerator KDN43VL40I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Bosch 347 L Frost Free Double Door 3 Star Refrigerator KDN43VL40I\n",
      "Augmented Text:\n",
      "['Max Bosch 347 L and Frost Road Free Car Double Opened Door Section 3 Bronze Star Service Refrigerator KDN43VL40I']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-cased', action=\"insert\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Bosch 347 L Frost Free Double Door 3 Star Refrigerator KDN43VL40I\n",
      "Augmented Text:\n",
      "['303 36 mm Frost Free Side Door Four Year Mark KDN43VL40I']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-cased', action=\"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Bosch 347 L Frost Free Double Door 3 Star Refrigerator KDN43VL40I\n",
      "Augmented Text:\n",
      "['Bosch 46 L 76 Mark 9 inch 8 Star machine gun']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-cased', action=\"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Bosch 347 L Frost Free Double Door 3 Star Refrigerator KDN43VL40I\n",
      "Augmented Text:\n",
      "['Bosch 347 L Frost Free Double Door 3 Star Refrigerator KDN43VL40I , in to 1 \" ( - ( is are .']\n"
     ]
    }
   ],
   "source": [
    "aug = nas.ContextualWordEmbsForSentenceAug(model_path='gpt2')\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# model_path: xlnet-base-cased or gpt2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m aug \u001b[39m=\u001b[39m nas\u001b[39m.\u001b[39mContextualWordEmbsForSentenceAug(model_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mxlnet-base-cased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m augmented_texts \u001b[39m=\u001b[39m aug\u001b[39m.\u001b[39;49maugment(text, n\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOriginal:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(text)\n",
      "File \u001b[1;32mc:\\Users\\Ayush Bhauwala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nlpaug\\base_augmenter.py:98\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, data, n, num_thread)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mAbstSummAug\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBackTranslationAug\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mContextualWordEmbsAug\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mContextualWordEmbsForSentenceAug\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     97\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(aug_num):\n\u001b[1;32m---> 98\u001b[0m         result \u001b[39m=\u001b[39m action_fx(clean_data)\n\u001b[0;32m     99\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    100\u001b[0m             augmented_results\u001b[39m.\u001b[39mextend(result)\n",
      "File \u001b[1;32mc:\\Users\\Ayush Bhauwala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nlpaug\\augmenter\\sentence\\context_word_embs_sentence.py:116\u001b[0m, in \u001b[0;36mContextualWordEmbsForSentenceAug.insert\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    113\u001b[0m     all_data \u001b[39m=\u001b[39m [data]\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_custom_api:\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_custom_insert(all_data)\n\u001b[0;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_native_insert(all_data)\n",
      "File \u001b[1;32mc:\\Users\\Ayush Bhauwala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nlpaug\\augmenter\\sentence\\context_word_embs_sentence.py:191\u001b[0m, in \u001b[0;36mContextualWordEmbsForSentenceAug._custom_insert\u001b[1;34m(self, all_data)\u001b[0m\n\u001b[0;32m    189\u001b[0m     results \u001b[39m=\u001b[39m [d \u001b[39m+\u001b[39m a \u001b[39mfor\u001b[39;00m d, a \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(all_data, augmented_texts)]\n\u001b[0;32m    190\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mxlnet\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m--> 191\u001b[0m     results \u001b[39m=\u001b[39m [d \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mconvert_tokens_to_string(a) \u001b[39mfor\u001b[39;49;00m d, a \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(all_data, augmented_texts)]\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\Ayush Bhauwala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nlpaug\\augmenter\\sentence\\context_word_embs_sentence.py:191\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    189\u001b[0m     results \u001b[39m=\u001b[39m [d \u001b[39m+\u001b[39m a \u001b[39mfor\u001b[39;00m d, a \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(all_data, augmented_texts)]\n\u001b[0;32m    190\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mxlnet\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m--> 191\u001b[0m     results \u001b[39m=\u001b[39m [d \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mconvert_tokens_to_string(a) \u001b[39mfor\u001b[39;00m d, a \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(all_data, augmented_texts)]\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\Ayush Bhauwala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:533\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_tokens_to_string\u001b[39m(\u001b[39mself\u001b[39m, tokens: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend_tokenizer\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mdecode(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 'tokens': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "source": [
    "# model_path: xlnet-base-cased or gpt2\n",
    "aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')\n",
    "augmented_texts = aug.augment(text, n=3)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "print(augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
